{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try FitBert.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Onamihoang/NLP-IELTS/blob/master/Try_FitBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kmGJXc_8CCw",
        "colab_type": "text"
      },
      "source": [
        "# FitBert\n",
        "\n",
        "[FitBert](https://github.com/Qordobacode/fitbert) ((F)ill (i)n (t)he blanks, (BERT)) is a library for using BERT to fill in the blank(s) in a section of text from a list of options.\n",
        "\n",
        "It's easy to use, just install with pip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX-j2onbkEyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8f85d57-3e28-439f-d153-3b6c24e45c13"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytMNc45x7_Hf",
        "colab_type": "code",
        "outputId": "9c3174db-ae66-4b7d-c017-96ac2f702712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "!pip install fitbert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fitbert in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: transformers>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from fitbert) (2.8.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from fitbert) (1.5.0+cu101)\n",
            "Requirement already satisfied: PyFunctional==1.2.0 in /usr/local/lib/python3.6/dist-packages (from fitbert) (1.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (1.18.3)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (0.5.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (1.12.47)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (0.1.86)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->fitbert) (3.0.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->fitbert) (0.16.0)\n",
            "Requirement already satisfied: tabulate<=1.0.0 in /usr/local/lib/python3.6/dist-packages (from PyFunctional==1.2.0->fitbert) (0.8.7)\n",
            "Requirement already satisfied: dill==0.2.5 in /usr/local/lib/python3.6/dist-packages (from PyFunctional==1.2.0->fitbert) (0.2.5)\n",
            "Requirement already satisfied: six<=2.0.0 in /usr/local/lib/python3.6/dist-packages (from PyFunctional==1.2.0->fitbert) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.1.1->fitbert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.1.1->fitbert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.1.1->fitbert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.1.1->fitbert) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->fitbert) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->fitbert) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->fitbert) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->fitbert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->fitbert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers>=2.1.1->fitbert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers>=2.1.1->fitbert) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBXqjMehH5EG",
        "colab_type": "text"
      },
      "source": [
        "Then import and use it (note - this requires downloading and loading into memory a pretrained BERT model and takes a minute or two):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1lwWl12H0ko",
        "colab_type": "code",
        "outputId": "200db3cd-0406-4a87-a23e-6d4026095caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from fitbert import FitBert\n",
        "\n",
        "\n",
        "# in theory you can pass a model_name and tokenizer, but currently only\n",
        "# bert-large-uncased and BertTokenizer are available\n",
        "# this takes a while and loads a whole big BERT into memory\n",
        "fb = FitBert()\n",
        "\n",
        "masked_string = \"Why Bert, you're looking ***mask*** today!\"\n",
        "options = ['buff', 'handsome', 'strong']\n",
        "\n",
        "ranked_options = fb.rank(masked_string, options=options)\n",
        "ranked_options"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using model: bert-large-uncased\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['handsome', 'strong', 'buff']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Tjs0_kIXPC",
        "colab_type": "code",
        "outputId": "a7ae89d4-344a-421c-b525-540232deb9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filled_in = fb.fitb(masked_string, options=options)\n",
        "filled_in"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why Bert, you're looking handsome today!\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXrkQhyvJPAP",
        "colab_type": "text"
      },
      "source": [
        "There's a convenience method for masking a span (and filling in the suggestion, or not):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqoFqNCTJFBH",
        "colab_type": "code",
        "outputId": "e4cfacec-3fb0-4317-fbad-e9d6e4275cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unmasked_string = \"Why Bert, you're looks handsome today!\"\n",
        "span_to_mask = (17, 22)\n",
        "\n",
        "filled_in = fb.mask_fitb(unmasked_string, span_to_mask)\n",
        "filled_in"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why Bert, you're  looking  handsome today!\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmy8vk-uJUTS",
        "colab_type": "code",
        "outputId": "7c36e709-f43b-4c4d-d175-daa223854450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "masked_string, masked = fb.mask(unmasked_string, span_to_mask)\n",
        "print(masked_string, masked)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Why Bert, you're  ***mask***  handsome today! looks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2p_ssDQLzjP",
        "colab_type": "text"
      },
      "source": [
        "## From the \"Introducing FitBERT\" blog post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxQ_v2kLNlwA",
        "colab_type": "text"
      },
      "source": [
        "### SWE section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u6IFt8ckdA4",
        "colab_type": "code",
        "outputId": "56471536-483a-49aa-9037-9176a5085141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "masked_string = \"Your 6 ***mask*** sodas are on their way !\"\n",
        "options = ['hot', 'cold', 'sweet', 'delicious', 'artisanal']\n",
        "fb.fitb(masked_string, options=options)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your 6 cold sodas are on their way !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFBWmulHL6Jl",
        "colab_type": "code",
        "outputId": "205412e5-cc20-4272-890f-53d91b68a177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "masked_string = \"Your 17 ***mask*** burritos are on their way !\"\n",
        "options = ['hot', 'cold', 'sweet', 'delicious', 'artisanal']\n",
        "fb.fitb(masked_string, options=options)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your 17 delicious burritos are on their way !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjnksdnNpXh",
        "colab_type": "text"
      },
      "source": [
        "### Researcher section\n",
        "\n",
        "One use case for FitBERT is easily evaluating the syntactic capabilities of any model available through the [Transformers library](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftransformers), which includes BERT, RoBERTa, GPT2, and DistillBERT.\n",
        "\n",
        "This is very similar to Yoav GoldBerg's [Assessing BERT's Syntactic Abilities](https://arxiv.org/abs/1901.05287). AFAIK, this experiment hasn't been repeated with RoBERTa or DistillBERT, but would be interesting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjfAmIwGMwAe",
        "colab_type": "code",
        "outputId": "c98354e0-fd44-491a-9300-bfa8594ac521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example from \"Targeted Syntactic Evaluation of Language Models\"\n",
        "# https://arxiv.org/abs/1808.09031\n",
        "\n",
        "masked_string = \"the author that the guard likes ***mask***\"\n",
        "options = ['laugh', 'laughs']\n",
        "fb.rank_with_prob(masked_string, options)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['laughs', 'laugh'], [4.141843985838722e-12, 3.374725358103875e-13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adNVTDKMOr84",
        "colab_type": "code",
        "outputId": "b820b171-2ffd-4a57-a944-b4f0fc9f9ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example from \"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies\"\n",
        "# https://transacl.org/ojs/index.php/tacl/article/view/972\n",
        "\n",
        "masked_string = \"accusations of abusive sockpuppetry from a trusted source ***mask*** a serious chilling effect .\"\n",
        "options = [\"have\", \"has\"]\n",
        "fb.rank_with_prob(masked_string, options)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['have', 'has'], [0.8899506330490112, 0.004103281069546938])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkqpv2FYKwwL",
        "colab_type": "text"
      },
      "source": [
        "### Using FitBERT with a spell corrector\n",
        "\n",
        "Example of refining the output of a [word-vector-based spell checker](https://blog.usejournal.com/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26) with BERT. This would also work with something like Hunspell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sTtRg_SLXpE",
        "colab_type": "code",
        "outputId": "3ab13f0c-c3ab-4f16-87e5-13b4cc41b293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = \"We predict the following issues will ocur.\"\n",
        "masked_string = \"We predict the following issues will ***mask*** .\"\n",
        "# mispelling vector subtraction gives the following options for \"ocur\"\n",
        "options = ['ocur', 'occur', 'arise', 'happen', 'reliably']\n",
        "fb.fitb(masked_string, options=options)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We predict the following issues will arise .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Bg5yaOi5Zi",
        "colab_type": "code",
        "outputId": "bf6515aa-50e9-4630-a824-7381a8ca5afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = \"We predict the following issues will ocur.\"\n",
        "masked_string = \"We predict the following issues will ***mask*** .\"\n",
        "# mispelling vector subtraction gives the following options for \"ocur\", but filter through Levenshtein distance threshold:\n",
        "options = ['ocur', 'occur']\n",
        "fb.fitb(masked_string, options=options)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We predict the following issues will occur .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek0DoVsHGLa",
        "colab_type": "text"
      },
      "source": [
        "### (Work in Progress) Using FiTBERT for truecasing\n",
        "\n",
        "An efficient implementation will require:\n",
        "\n",
        "1. Fixing the bug where probabilities returned by `fb.rank(with_prob=True)` aren't in the same order as the tokens returned\n",
        "2. Tensorizing the handling of multi-token masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQkTzRVwLiFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-GAL3I98v6m",
        "colab_type": "code",
        "outputId": "a07f2f73-3fcb-4762-8544-393102981c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "new_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "new_bert = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
        "\n",
        "fb2 = FitBert(model=new_bert, tokenizer=new_tokenizer, disable_gpu=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using model: bert-large-uncased\n",
            "device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9HTQpKP-MUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_case(word: str):\n",
        "    if not word.isalpha():\n",
        "        return False\n",
        "\n",
        "    if word.lower() == word:\n",
        "        return word.capitalize()\n",
        "    elif word.capitalize() == word:\n",
        "        return word.lower()\n",
        "    else:\n",
        "        # camelCase, ALLUPPER, sPoNGeBoB, etc\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XopjyyH9AjQ",
        "colab_type": "code",
        "outputId": "a1d07c85-21df-47e3-8c7d-8249a8e17c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Naive implementation handles some cases\n",
        "masked_string = f'{fb2.mask_token} more than 2000 minerals are known, nearly all rocks are formed from seven mineral groups.'\n",
        "'''\n",
        "#(A) Although \n",
        "(B) However \n",
        "(C) Despite \n",
        "(D) Since   \n",
        "'''\n",
        "#masked_string = f\"These {fb2.mask_token} some common grammatical mistakes .\"\n",
        "options = [\"Although\", \"However\", \"Despite\", \"Since\" ]\n",
        "fb.rank_with_prob(masked_string, options=options)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Although', 'Since', 'Despite', 'However'],\n",
              " [0.7068175673484802,\n",
              "  0.007502323482185602,\n",
              "  0.0011061042314395308,\n",
              "  0.0002121678408002481])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHTzrVjrnomq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "15b7ec78-501a-408f-cfde-194c018c4170"
      },
      "source": [
        "masked_string = f\"Our friends are expected to assume the burden of their own defense,{fb2.mask_token} they are competent to do.\"\n",
        "options = [\"which we are certain\", \"that we are certain of \", \"of which we are sure\", \"for which we are sure\" ]\n",
        "fb.rank_with_prob(masked_string, options=options)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['which we are certain',\n",
              "  'of which we are sure',\n",
              "  'for which we are sure',\n",
              "  'that we are certain of'],\n",
              " [7.986267282007622e-14,\n",
              "  8.050572642896491e-15,\n",
              "  3.708180255270946e-15,\n",
              "  5.097586574393146e-21])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N9fr_goG9mY",
        "colab_type": "code",
        "outputId": "2400e304-f4a1-48a2-d13c-f11a5429a35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# but not others\n",
        "masked_string = f\"These are some Common {fb2.mask_token} mistakes .\"\n",
        "options = [\"grammatical\", \"Grammatical\"]\n",
        "fb2.fitb(masked_string, options=options)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'These are some Common Grammatical mistakes .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcGBJ79PCmwA",
        "colab_type": "code",
        "outputId": "3aa11e38-b224-4da6-f9f6-67717f9fdca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# we can loop through and look at each pairwise comparison to see whats going on\n",
        "# except we can't trust the probabilities order, because of a bug in fitbert\n",
        "orig = \"These are some Common Grammatical mistakes .\"\n",
        "\n",
        "tokens = orig.split(\" \")\n",
        "for i, token in enumerate(tokens):\n",
        "    masked_string = \" \".join(tokens[:i]) + fb2.mask_token + \" \".join(tokens[i:])\n",
        "    changed = change_case(token)\n",
        "    if changed:\n",
        "        options = [changed, token]\n",
        "        ranked, probs = fb2.rank(masked_string, options, with_prob=True)\n",
        "        print(ranked, probs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['These', 'these'] [0.00013668823521584272, 1.906783836602699e-05]\n",
            "['are', 'Are'] [0.0012963397894054651, 1.410887534802896e-06]\n",
            "['some', 'Some'] [0.0023924994748085737, 9.767485607881099e-06]\n",
            "['common', 'Common'] [0.05046175420284271, 0.026039525866508484]\n",
            "['Grammatical', 'grammatical'] [0.0001627308374736458, 3.978714630648028e-06]\n",
            "['mistakes', 'Mistakes'] [0.00013252785720396787, 2.397542289145349e-07]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asv7A-3hFh73",
        "colab_type": "code",
        "outputId": "f2a0a4e4-3184-4ee0-ee5b-527aa1b30ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Using greedy decoding works pretty well\n",
        "\n",
        "orig = \"These are some Common Grammatical mistakes .\"\n",
        "\n",
        "tokens = orig.split(\" \")\n",
        "for i, token in enumerate(tokens):\n",
        "    if token.isalpha():\n",
        "        masked_string = \" \".join(tokens[0:i]) + \" \" + fb2.mask_token + \" \" + \" \".join(tokens[i + 1:])\n",
        "        print(\"the masked string is: \", masked_string)\n",
        "        changed = change_case(token)\n",
        "        if changed:\n",
        "            options = [changed, token]\n",
        "            ranked, probs = fb2.rank(masked_string, options, with_prob=True)\n",
        "            print(ranked, probs)\n",
        "            if ranked[0] == changed:\n",
        "                # should use probs, but there is a bug where it is sorted before being returned :facepalm:\n",
        "                tokens[i] = changed\n",
        "                print(\"the string is now:\", \" \".join(tokens))\n",
        "print(\"final version is\")\n",
        "print(\" \".join(tokens))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the masked string is:   ***mask*** are some Common Grammatical mistakes .\n",
            "['These', 'these'] [0.052241019904613495, 0.0011475221253931522]\n",
            "the masked string is:  These ***mask*** some Common Grammatical mistakes .\n",
            "['are', 'Are'] [0.49185436964035034, 0.0001234041847055778]\n",
            "the masked string is:  These are ***mask*** Common Grammatical mistakes .\n",
            "['some', 'Some'] [0.006553380750119686, 0.000497280212584883]\n",
            "the masked string is:  These are some ***mask*** Grammatical mistakes .\n",
            "['common', 'Common'] [0.2282126545906067, 0.011387933045625687]\n",
            "the string is now: These are some common Grammatical mistakes .\n",
            "the masked string is:  These are some common ***mask*** mistakes .\n",
            "['grammatical', 'Grammatical'] [1.6586891433689743e-05, 1.957106633199146e-06]\n",
            "the string is now: These are some common grammatical mistakes .\n",
            "the masked string is:  These are some common grammatical ***mask*** .\n",
            "['mistakes', 'Mistakes'] [0.0025533498264849186, 5.693058025002529e-09]\n",
            "final version is\n",
            "These are some common grammatical mistakes .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k3x9IoIJe7Q",
        "colab_type": "code",
        "outputId": "c4d6f52f-0fd6-4104-baf9-10806ec802e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# I can't tell if this behaviour is ok\n",
        "# Wikipedia says that this correction is good\n",
        "# I thought this would be hard for the model...\n",
        "\n",
        "orig = \"I 'm really feeling Panic! At The Disco .\"\n",
        "\n",
        "tokens = orig.split(\" \")\n",
        "for i, token in enumerate(tokens):\n",
        "    if i>0:\n",
        "        if token.isalpha():\n",
        "            masked_string = \" \".join(tokens[0:i]) + \" \" + fb2.mask_token + \" \" + \" \".join(tokens[i + 1:])\n",
        "            changed = change_case(token)\n",
        "            if changed:\n",
        "                options = [changed, token]\n",
        "                ranked, probs = fb2.rank(masked_string, options, with_prob=True)\n",
        "                if ranked[0] == changed:\n",
        "                    # should use probs, but there is a bug where it is sorted before being returned :facepalm:\n",
        "                    tokens[i] = changed\n",
        "print(\"final version is\")\n",
        "print(\" \".join(tokens))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final version is\n",
            "I 'm really feeling Panic! at the Disco .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rCzD68cLFbP",
        "colab_type": "code",
        "outputId": "bdb5d437-ece2-4acf-826c-ee23a84a2d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Truecasing is nearly impossible if a product name is also a common noun\n",
        "\n",
        "orig = \"Create Styleguides to standardize a writing style across all your content — or to manage distinct styles for different audiences .\"\n",
        "\n",
        "tokens = orig.split(\" \")\n",
        "for i, token in enumerate(tokens):\n",
        "    if token.isalpha() and i>0:\n",
        "        masked_string = \" \".join(tokens[0:i]) + \" \" + fb2.mask_token + \" \" + \" \".join(tokens[i + 1:])\n",
        "        changed = change_case(token)\n",
        "        if changed:\n",
        "            options = [changed, token]\n",
        "            ranked, probs = fb2.rank(masked_string, options, with_prob=True)\n",
        "            if ranked[0] == changed:\n",
        "                # should use probs, but there is a bug where it is sorted before being returned :facepalm:\n",
        "                tokens[i] = changed\n",
        "print(\"final version is\")\n",
        "print(\" \".join(tokens))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final version is\n",
            "Create styleguides to standardize a writing style across all your content — or to manage distinct styles for different audiences .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLxKCCrsLyV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}